# 使用pytorch求梯度
下图是参考资料1的一个习题.求一个函数在指定位置的梯度.先根据梯度的定义人工计算.
![](https://images.cnblogs.com/cnblogs_com/zhouyang209117/757815/o_220324010124_1.jpg)
因为
$$
\begin{equation*}
   \frac{\partial f}{\partial x}=2x+y+3, \frac{\partial f}{\partial y}=4y+x-2, \frac{\partial f}{\partial z}=6z-6
\end{equation*}
$$
所以
$$
\begin{equation*}
   \textbf{grad} f(x,y,z)=(2x+y+3)\bm{i}+(4y+x-2)\bm{j}+(6z-6)\bm{k}
\end{equation*}
$$
那么
$$
\begin{equation*}
   \bigtriangledown f(0,0,0)=3\bm{i}-2\bm{j}-6\bm{k}=(3,-2,-6)
\end{equation*}
$$

$$
\begin{equation*}
   \bigtriangledown f(1,1,1)=6\bm{i}+3\bm{j}=(3,-2,0)
\end{equation*}
$$
再使用pytorch计算
```
# coding: utf-8
import torch


def grad(x, y, z):
    x = torch.tensor([x], requires_grad=True)
    y = torch.tensor([y], requires_grad=True)
    z = torch.tensor([z], requires_grad=True)
    f_value = x * x + 2 * y * y + 3 * z * z + x * y + 3 * x - 2 * y - 6 * z
    f_value.backward()
    return x.grad.item(), y.grad.item(), z.grad.item()


if __name__ == '__main__':
    print(grad(0.0, 0.0, 0.0))
    print(grad(1.0, 1.0, 1.0))

```
计算结果和人工计算是一样的.特别要注意backward函数,执行这个函数等于帮我们求了梯度，不用再人工计算梯度,大大减少了工作量.
## 参考资料
1. 同济大学数学系,高等数学第七版下册,高等教育出版社,p103-111, 2015. 
1. [深度学习初级入门基础教程](https://edu.51cto.com/course/10037.html)